{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"InterLink NATS Plugin","title":"InterLink NATS Plugin"},{"location":"#interlink-nats-plugin","text":"","title":"InterLink NATS Plugin"},{"location":"architecture/","text":"Architecture","title":"Architecture"},{"location":"architecture/#architecture","text":"","title":"Architecture"},{"location":"deploy/","text":"Deployment with Helm3","title":"Deployment with Helm"},{"location":"deploy/#deployment-with-helm3","text":"","title":"Deployment with Helm3"},{"location":"concepts/authentication/","text":"Authentication","title":"Authentication"},{"location":"concepts/authentication/#authentication","text":"","title":"Authentication"},{"location":"concepts/build-config/","text":"Build Configuration","title":"Build Configuration"},{"location":"concepts/build-config/#build-configuration","text":"","title":"Build Configuration"},{"location":"concepts/git-sync/","text":"Git Sync","title":"Git Sync"},{"location":"concepts/git-sync/#git-sync","text":"","title":"Git Sync"},{"location":"concepts/kueue-nats/","text":"Kueue integration ( MasterQueue ) The bidirectional messaging layer provided by NATS enables feedbacks on the status of the resource pools on the resources they may provide. To enable the definition of policies on the resource usage defined at cluster level (rather than at node level) the InterLink resource pools are mapped into kueue ResourceFlavor s and organized in ClusterQueue s . A custom controller, named kueue-nats , connects to the NATS server and subscribe to the subjects under which resource providers publish the available resources. When receiving resource updates, the kueue-nats controller creates or updates ResourceFlavor s the ClusterQueue s accordingly. The configuration of the kueue-nats controller relies on a Kubernetes Custom Resource (CR) named MasterQueue . A MasterQueue is supposed to be the cluster-level object collecting all the resource pools made available to the cluster either via InterLink or as local resources. The MasterQueue can then lend resources to groups of users or client applications using the Kueue resource-borrowing mechanism . All the ClusterQueue s spawned by a MasterQueue belong to the same cohort , named by the MasterQueue itself. Organizing Flavors As mentioned, ResourceFlavor s map resource pools. Since multiple resource pools might be equivalent and interchangeable to the applications running in the cluster (think for example of two Kubernetes clusters running in different sites), kueue-nats introduces groups of ResourceFlavor s, mapping groups of equivalent resource pools. Groups of resource flavors may fall in two categories: natsFlavor , managed by InterLink via the InterLink NATS plugin, or localFlavor , managed by the cluster administrator directly via Kueue and made available to the MasterQueue. natsFlavor s A natsFlavor should indicate the following properties: * natsConnector : a string defining the connection to the NATS server. If deploying the NATS plugin with the interlink-in-one , this string is provided by Helm itself when installing or updating the chart. * natsSubject : a string representing the NATS subject used by resource pools to publish updates on the available resources * virtualNode : is the name of the virtual node where to address pods to be submitted with interlink * poolTimeout : an integer defining the timeout in seconds. If no update to the available resources is obtained after this time interval, the corresponding resource flavor is dropped. The names of pools part of the group can be listed explicitly using the pools keyword, pools: - podman-firenze - slurm-cineca or it can be defined with a regular expression using the poolRegExp , for example poolRegExp: \"podman-.*\" Note Note that the regular expression is managed with the re module in Python. Minimal MasterQueue definition A minimal example of a MasterQueue is defined below with annotations apiVersion: vk.io/v1 kind: MasterQueue metadata: name: masterqueue # This is the name of the MasterQueue spec: template: cohort: masterqueue # This is the name of the cohort. It is suggested to use the same name as for the MasterQueue. flavors: # Here we introduce the list of resource flavors organized in categories. RFs can be either `natsFlavor`s or `localFlavor`s. - name: interlink # This is the name of the flavor category (not of the flavor itself!) natsFlavor: natsConnector: \"nats://user:password@nats.nats:4222\" natsSubject: interlink.resources virtualNode: interlink poolTimeout: 30 poolRegExp: \".*\" nominalQuota: pods: 100 canLendTo: - queue: group1 lendingLimit: pods: 30 This creates two ClusterQueue objects belonging to the same cohort masterqueue . One named masterqueue (as the MasterQueue object) has quotas based on the resources published by the various providers, the other, named group1 has no quota, but a borrowingLimit for pods set to 30. Warning The limits are specified per resource pool . In the example above, if two resource pools are available, the ClusterQueue group1 will be entitled to borrow 30 pods from the first one and 30 pods from the second. Creating a LocalQueue To connect a namespace to a ClusterQueue , the cluster admins must create LocalQueue objects. For example, to enable executing jobs in the default namespace in the group1 ClusterQueue, one may define the gr1 LocalQueue as apiVersion: kueue.x-k8s.io/v1beta1 kind: LocalQueue metadata: namespace: default name: gr1 spec: clusterQueue: group1 A test job To test the setup, one may submit a test job in the default namespace through the queue gr1 . apiVersion: batch/v1 kind: Job metadata: generateName: sample-job-1 namespace: default labels: kueue.x-k8s.io/queue-name: gr1 spec: parallelism: 1 completions: 1 suspend: true template: spec: containers: - name: dummy-job image: ghcr.io/grycap/cowsay:latest command: [\"/bin/sh\", \"-c\"] args: - | echo \"Hello world!\" sleep 30 resources: requests: cpu: 1 memory: \"200Mi\" tolerations: - key: virtual-node.interlink/no-schedule operator: Exists effect: NoSchedule restartPolicy: Never Note in particular: * the label kueue.x-k8s.io/queue-name: gr1 defining the local queue, * the spec suspend: true to let Kueue to submit the job once admitted, * the toleration to the node taint virtual-node.interlink/no-schedule marking the job as suitable for offloading. Local flavors To enable local execution of payloads not suitable for offloading, one may include a local flavor in the MasterQueue . Local flavors are a simply links to the standard ResourceFlavor objects of the Kueue that must be already defined in the cluster and fall outside the range of action of the kueue-nats controller. For example the following yaml manifest defines a ResourceFlavor and makes it available to the group1 ClusterQueue via the MasterQueue . apiVersion: kueue.x-k8s.io/v1beta1 kind: ResourceFlavor metadata: name: \"local-cpu\" # <-- Note this name --- apiVersion: vk.io/v1 kind: MasterQueue metadata: name: masterqueue spec: template: cohort: masterqueue flavors: - name: local-cpu # <-- This name should match the ResourceFlavor definition above localFlavor: {} nominalQuota: pods: 100 canLendTo: - queue: group1 lendingLimit: pods: 30","title":"Kueue integration (MasterQueue)"},{"location":"concepts/kueue-nats/#kueue-integration-masterqueue","text":"The bidirectional messaging layer provided by NATS enables feedbacks on the status of the resource pools on the resources they may provide. To enable the definition of policies on the resource usage defined at cluster level (rather than at node level) the InterLink resource pools are mapped into kueue ResourceFlavor s and organized in ClusterQueue s . A custom controller, named kueue-nats , connects to the NATS server and subscribe to the subjects under which resource providers publish the available resources. When receiving resource updates, the kueue-nats controller creates or updates ResourceFlavor s the ClusterQueue s accordingly. The configuration of the kueue-nats controller relies on a Kubernetes Custom Resource (CR) named MasterQueue . A MasterQueue is supposed to be the cluster-level object collecting all the resource pools made available to the cluster either via InterLink or as local resources. The MasterQueue can then lend resources to groups of users or client applications using the Kueue resource-borrowing mechanism . All the ClusterQueue s spawned by a MasterQueue belong to the same cohort , named by the MasterQueue itself.","title":"Kueue integration (MasterQueue)"},{"location":"concepts/kueue-nats/#organizing-flavors","text":"As mentioned, ResourceFlavor s map resource pools. Since multiple resource pools might be equivalent and interchangeable to the applications running in the cluster (think for example of two Kubernetes clusters running in different sites), kueue-nats introduces groups of ResourceFlavor s, mapping groups of equivalent resource pools. Groups of resource flavors may fall in two categories: natsFlavor , managed by InterLink via the InterLink NATS plugin, or localFlavor , managed by the cluster administrator directly via Kueue and made available to the MasterQueue.","title":"Organizing Flavors"},{"location":"concepts/kueue-nats/#natsflavors","text":"A natsFlavor should indicate the following properties: * natsConnector : a string defining the connection to the NATS server. If deploying the NATS plugin with the interlink-in-one , this string is provided by Helm itself when installing or updating the chart. * natsSubject : a string representing the NATS subject used by resource pools to publish updates on the available resources * virtualNode : is the name of the virtual node where to address pods to be submitted with interlink * poolTimeout : an integer defining the timeout in seconds. If no update to the available resources is obtained after this time interval, the corresponding resource flavor is dropped. The names of pools part of the group can be listed explicitly using the pools keyword, pools: - podman-firenze - slurm-cineca or it can be defined with a regular expression using the poolRegExp , for example poolRegExp: \"podman-.*\" Note Note that the regular expression is managed with the re module in Python.","title":"natsFlavors"},{"location":"concepts/kueue-nats/#minimal-masterqueue-definition","text":"A minimal example of a MasterQueue is defined below with annotations apiVersion: vk.io/v1 kind: MasterQueue metadata: name: masterqueue # This is the name of the MasterQueue spec: template: cohort: masterqueue # This is the name of the cohort. It is suggested to use the same name as for the MasterQueue. flavors: # Here we introduce the list of resource flavors organized in categories. RFs can be either `natsFlavor`s or `localFlavor`s. - name: interlink # This is the name of the flavor category (not of the flavor itself!) natsFlavor: natsConnector: \"nats://user:password@nats.nats:4222\" natsSubject: interlink.resources virtualNode: interlink poolTimeout: 30 poolRegExp: \".*\" nominalQuota: pods: 100 canLendTo: - queue: group1 lendingLimit: pods: 30 This creates two ClusterQueue objects belonging to the same cohort masterqueue . One named masterqueue (as the MasterQueue object) has quotas based on the resources published by the various providers, the other, named group1 has no quota, but a borrowingLimit for pods set to 30. Warning The limits are specified per resource pool . In the example above, if two resource pools are available, the ClusterQueue group1 will be entitled to borrow 30 pods from the first one and 30 pods from the second.","title":"Minimal MasterQueue definition"},{"location":"concepts/kueue-nats/#creating-a-localqueue","text":"To connect a namespace to a ClusterQueue , the cluster admins must create LocalQueue objects. For example, to enable executing jobs in the default namespace in the group1 ClusterQueue, one may define the gr1 LocalQueue as apiVersion: kueue.x-k8s.io/v1beta1 kind: LocalQueue metadata: namespace: default name: gr1 spec: clusterQueue: group1","title":"Creating a LocalQueue"},{"location":"concepts/kueue-nats/#a-test-job","text":"To test the setup, one may submit a test job in the default namespace through the queue gr1 . apiVersion: batch/v1 kind: Job metadata: generateName: sample-job-1 namespace: default labels: kueue.x-k8s.io/queue-name: gr1 spec: parallelism: 1 completions: 1 suspend: true template: spec: containers: - name: dummy-job image: ghcr.io/grycap/cowsay:latest command: [\"/bin/sh\", \"-c\"] args: - | echo \"Hello world!\" sleep 30 resources: requests: cpu: 1 memory: \"200Mi\" tolerations: - key: virtual-node.interlink/no-schedule operator: Exists effect: NoSchedule restartPolicy: Never Note in particular: * the label kueue.x-k8s.io/queue-name: gr1 defining the local queue, * the spec suspend: true to let Kueue to submit the job once admitted, * the toleration to the node taint virtual-node.interlink/no-schedule marking the job as suitable for offloading.","title":"A test job"},{"location":"concepts/kueue-nats/#local-flavors","text":"To enable local execution of payloads not suitable for offloading, one may include a local flavor in the MasterQueue . Local flavors are a simply links to the standard ResourceFlavor objects of the Kueue that must be already defined in the cluster and fall outside the range of action of the kueue-nats controller. For example the following yaml manifest defines a ResourceFlavor and makes it available to the group1 ClusterQueue via the MasterQueue . apiVersion: kueue.x-k8s.io/v1beta1 kind: ResourceFlavor metadata: name: \"local-cpu\" # <-- Note this name --- apiVersion: vk.io/v1 kind: MasterQueue metadata: name: masterqueue spec: template: cohort: masterqueue flavors: - name: local-cpu # <-- This name should match the ResourceFlavor definition above localFlavor: {} nominalQuota: pods: 100 canLendTo: - queue: group1 lendingLimit: pods: 30","title":"Local flavors"},{"location":"concepts/nats/","text":"NATS messaging layer","title":"NATS messaging layer"},{"location":"concepts/nats/#nats-messaging-layer","text":"","title":"NATS messaging layer"},{"location":"concepts/pilot/","text":"Pilot container","title":"Pilot Container"},{"location":"concepts/pilot/#pilot-container","text":"","title":"Pilot container"},{"location":"providers/condor/","text":"Condor provider Condor jobs are submitted through a Condor Compute Entrypoint authenticated through Javascript Web Tokens (JWT) provided by an Indigo IAM Instance. We report below recipes to configure the condor provider. Kubernetes The reported configuration works for INFN-T1 site. Modifications might be needed (especially for paths) in other sites. Then at some point we will have cvmfs. apiVersion: v1 kind: ConfigMap metadata: name: condor-sub-build-conf data: interlink.conf: | # Volumes and directories configuring access to executor-local data [volumes] ### Area in the executor filesystem to be used for temporary files scratch_area = \"/tmp\" ### Location to cache singularity images in the executor filesystem apptainer_cachedir = \"/tmp/cache/apptainer\" ### Location where to look for pre-built images image_dir = \"/opt/exp_software/opssw/budda\" ### Colon-separated list of directories to include in $PATH to look for executables additional_directories_in_path = [\"/opt/exp_software/opssw/mabarbet/bin\"] # Options configuring the behavior of apptainer runtime [apptainer] ### Enables --fakeroot in apptainer exec/run commands fakeroot = false ### Enables --containall flag in apptainer exec/run commands containall = false ### Defines whether the host enables users to mount fuse volumes or not fuse_mode = \"host-privileged\" ### Do not propagate umask to the container, set default 0022 umask no_init = false ### Do not bind home by default no_home = true ### Drop all privileges from root user in container no_privs = true ### Enable nVidia GPU support nvidia_support = false ### Clean the environment of the spawned container cleanenv = true # Proxy to download pre-build Docker Images instead of rebuilding at each execution [shub_proxy] ### shub proxy instance used to retrieve cached singularity images. Without protocol! server = \"<insert shub-proxy endpoint>\" ### Master token of the proxy used to request and generate client tokens master_token = \"<insert shub-proxy master token>\" interlink.sh: | echo \"Cloning the repo\" git clone --quiet --branch main https://github.com/landerlini/interlink-condorce-plugin plugin &> log echo \"Repo cloned\" cd plugin mkdir -p /opt/interlink echo \"Starting authentication procedure\" python3 -u natsprovider/tests/_auth.py /opt/interlink/refresh_token export REFRESH_TOKEN=$(cat /opt/interlink/refresh_token) while true do git pull &>> log python3 -um natsprovider condor \\ --queue \"infncnaf\" \\ --shutdown-subject \"*\" \\ --non-interactive done --- apiVersion: apps/v1 kind: Deployment metadata: name: condor-submitter labels: app: interlink component: condor-submitter spec: replicas: 1 selector: matchLabels: app: interlink component: condor-submitter template: metadata: labels: app: interlink component: condor-submitter spec: securityContext: fsGroup: 101 containers: - name: condorprovider image: \"landerlini/interlink-condorce-plugin:v0.1.2\" command: [\"/bin/bash\", \"-u\", \"/etc/interlink/interlink.sh\"] env: - name: DEBUG value: \"true\" - name: TOKEN_VALIDITY_SECONDS value: \"1200\" - name: _condor_CONDOR_HOST value: \"ce01t-htc.cr.cnaf.infn.it:9619\" - name: CONDOR_POOL value: \"ce01t-htc.cr.cnaf.infn.it:9619\" - name: _condor_SCHEDD_HOST value: \"ce01t-htc.cr.cnaf.infn.it\" - name: CONDOR_SCHEDULER_NAME value: \"ce01t-htc.cr.cnaf.infn.it\" - name: IAM_ISSUER value: \"https://iam-t1-computing.cloud.cnaf.infn.it\" - name: IAM_CLIENT_ID value: <iam client id> - name: IAM_CLIENT_SECRET value: <iam client secret> - name: NATS_SUBJECT value: \"interlink\" - name: NATS_SERVER value: <nats server connector> - name: NATS_TIMEOUT_SECONDS value: \"60\" - name: DEFAULT_ALLOCATABLE_CPU value: \"1\" - name: DEFAULT_ALLOCATABLE_MEMORY value: \"2Gi\" - name: DEFAULT_ALLOCATABLE_PODS value: \"10\" - name: DEFAULT_ALLOCATABLE_GPUS value: \"0\" volumeMounts: - name: condorplugin-conf mountPath: /etc/interlink readOnly: true volumes: - name: condorplugin-conf configMap: name: condor-sub-build-conf items: - key: interlink.conf path: build.conf - key: interlink.sh path: interlink.sh","title":"Condor"},{"location":"providers/condor/#condor-provider","text":"Condor jobs are submitted through a Condor Compute Entrypoint authenticated through Javascript Web Tokens (JWT) provided by an Indigo IAM Instance. We report below recipes to configure the condor provider.","title":"Condor provider"},{"location":"providers/condor/#kubernetes","text":"The reported configuration works for INFN-T1 site. Modifications might be needed (especially for paths) in other sites. Then at some point we will have cvmfs. apiVersion: v1 kind: ConfigMap metadata: name: condor-sub-build-conf data: interlink.conf: | # Volumes and directories configuring access to executor-local data [volumes] ### Area in the executor filesystem to be used for temporary files scratch_area = \"/tmp\" ### Location to cache singularity images in the executor filesystem apptainer_cachedir = \"/tmp/cache/apptainer\" ### Location where to look for pre-built images image_dir = \"/opt/exp_software/opssw/budda\" ### Colon-separated list of directories to include in $PATH to look for executables additional_directories_in_path = [\"/opt/exp_software/opssw/mabarbet/bin\"] # Options configuring the behavior of apptainer runtime [apptainer] ### Enables --fakeroot in apptainer exec/run commands fakeroot = false ### Enables --containall flag in apptainer exec/run commands containall = false ### Defines whether the host enables users to mount fuse volumes or not fuse_mode = \"host-privileged\" ### Do not propagate umask to the container, set default 0022 umask no_init = false ### Do not bind home by default no_home = true ### Drop all privileges from root user in container no_privs = true ### Enable nVidia GPU support nvidia_support = false ### Clean the environment of the spawned container cleanenv = true # Proxy to download pre-build Docker Images instead of rebuilding at each execution [shub_proxy] ### shub proxy instance used to retrieve cached singularity images. Without protocol! server = \"<insert shub-proxy endpoint>\" ### Master token of the proxy used to request and generate client tokens master_token = \"<insert shub-proxy master token>\" interlink.sh: | echo \"Cloning the repo\" git clone --quiet --branch main https://github.com/landerlini/interlink-condorce-plugin plugin &> log echo \"Repo cloned\" cd plugin mkdir -p /opt/interlink echo \"Starting authentication procedure\" python3 -u natsprovider/tests/_auth.py /opt/interlink/refresh_token export REFRESH_TOKEN=$(cat /opt/interlink/refresh_token) while true do git pull &>> log python3 -um natsprovider condor \\ --queue \"infncnaf\" \\ --shutdown-subject \"*\" \\ --non-interactive done --- apiVersion: apps/v1 kind: Deployment metadata: name: condor-submitter labels: app: interlink component: condor-submitter spec: replicas: 1 selector: matchLabels: app: interlink component: condor-submitter template: metadata: labels: app: interlink component: condor-submitter spec: securityContext: fsGroup: 101 containers: - name: condorprovider image: \"landerlini/interlink-condorce-plugin:v0.1.2\" command: [\"/bin/bash\", \"-u\", \"/etc/interlink/interlink.sh\"] env: - name: DEBUG value: \"true\" - name: TOKEN_VALIDITY_SECONDS value: \"1200\" - name: _condor_CONDOR_HOST value: \"ce01t-htc.cr.cnaf.infn.it:9619\" - name: CONDOR_POOL value: \"ce01t-htc.cr.cnaf.infn.it:9619\" - name: _condor_SCHEDD_HOST value: \"ce01t-htc.cr.cnaf.infn.it\" - name: CONDOR_SCHEDULER_NAME value: \"ce01t-htc.cr.cnaf.infn.it\" - name: IAM_ISSUER value: \"https://iam-t1-computing.cloud.cnaf.infn.it\" - name: IAM_CLIENT_ID value: <iam client id> - name: IAM_CLIENT_SECRET value: <iam client secret> - name: NATS_SUBJECT value: \"interlink\" - name: NATS_SERVER value: <nats server connector> - name: NATS_TIMEOUT_SECONDS value: \"60\" - name: DEFAULT_ALLOCATABLE_CPU value: \"1\" - name: DEFAULT_ALLOCATABLE_MEMORY value: \"2Gi\" - name: DEFAULT_ALLOCATABLE_PODS value: \"10\" - name: DEFAULT_ALLOCATABLE_GPUS value: \"0\" volumeMounts: - name: condorplugin-conf mountPath: /etc/interlink readOnly: true volumes: - name: condorplugin-conf configMap: name: condor-sub-build-conf items: - key: interlink.conf path: build.conf - key: interlink.sh path: interlink.sh","title":"Kubernetes"},{"location":"providers/kubernetes/","text":"Kubernetes provider","title":"Kubernetes"},{"location":"providers/kubernetes/#kubernetes-provider","text":"","title":"Kubernetes provider"},{"location":"providers/podman/","text":"Podman provider Warning Note! At the moment running with podman requires root privileges.","title":"Podman"},{"location":"providers/podman/#podman-provider","text":"Warning Note! At the moment running with podman requires root privileges.","title":"Podman provider"},{"location":"providers/slurm/","text":"Slurm provider","title":"Slurm"},{"location":"providers/slurm/#slurm-provider","text":"","title":"Slurm provider"}]}